import{_ as r,c as a,o as n,al as o}from"./chunks/framework.DQyQtHfj.js";const h=JSON.parse('{"title":"References","description":"","frontmatter":{},"headers":[],"relativePath":"references.md","filePath":"references.md","lastUpdated":null}'),t={name:"references.md"};function i(l,e,s,d,p,g){return n(),a("div",null,[...e[0]||(e[0]=[o('<h1 id="References" tabindex="-1">References <a class="header-anchor" href="#References" aria-label="Permalink to &quot;References {#References}&quot;">​</a></h1><ol><li><p><a id="greydanus2019hamiltonian"></a> S. Greydanus, M. Dzamba and J. Yosinski. <em>Hamiltonian Neural Networks</em>. In: <em>Proceedings of the 33rd International Conference on Neural Information Processing Systems</em> (Curran Associates Inc., Vancouver, 2019); pp. 15379–15389.</p></li><li><p><a id="luo2025transolver"></a> H. Luo, H. Wu, H. Zhou, L. Xing, Y. Di, J. Wang and M. Long. <a href="https://doi.org/10.48550/arXiv.2502.02414" target="_blank" rel="noreferrer"><em>Transolver++: An Accurate Neural Solver for PDEs on Million-Scale Geometries</em></a> (Feb 2025), <a href="https://arxiv.org/abs/2502.02414" target="_blank" rel="noreferrer">arXiv:2502.02414</a>.</p></li><li><p><a id="wu2024transolver"></a> H. Wu, H. Luo, H. Wang, J. Wang and M. Long. <a href="https://proceedings.mlr.press/v235/wu24r.html" target="_blank" rel="noreferrer"><em>Transolver: A Fast Transformer Solver for PDEs on General Geometries</em></a>. In: <em>Proceedings of the 41st International Conference on Machine Learning</em>, <em>ICML&#39;24</em> (JMLR, Vienna, 2024).</p></li><li><p><a id="gaby2022lyapunovnetdeepneuralnetwork"></a> N. Gaby, F. Zhang and X. Ye. <a href="https://ieeexplore.ieee.org/document/9993006" target="_blank" rel="noreferrer"><em>Lyapunov-Net: A Deep Neural Network Architecture for Lyapunov Function Approximation</em></a>. In: <a href="https://doi.org/10.1109/cdc51059.2022.9993006" target="_blank" rel="noreferrer"><em>2022 IEEE 61st Conference on Decision and Control (CDC)</em></a> (IEEE, Cancun, Dec 2022); pp. 2091–2096.</p></li><li><p><a id="dosovitskiy2020image"></a> A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit and N. Houlsby. <a href="https://doi.org/10.48550/arXiv.2010.11929" target="_blank" rel="noreferrer"><em>An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale</em></a> (Jun 2021), <a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noreferrer">arXiv:2010.11929</a>.</p></li><li><p><a id="krizhevsky2012imagenet"></a> A. Krizhevsky, I. Sutskever and G. E. Hinton. <em>ImageNet Classification with Deep Convolutional Neural Networks</em>. <a href="https://doi.org/10.1145/3065386" target="_blank" rel="noreferrer">Communications of the ACM <strong>60</strong>, 84–90</a> (2017).</p></li><li><p><a id="tan2019efficientnet"></a> M. Tan and Q. Le. <a href="https://proceedings.mlr.press/v97/tan19a.html" target="_blank" rel="noreferrer"><em>Efficientnet: Rethinking Model Scaling for Convolutional Neural Networks</em></a>. In: <em>Proceedings of the 36th International Conference on Machine Learning</em>, Vol. 97 (PMLR, PMLR, 2019); pp. 6105–6114.</p></li><li><p><a id="simonyan2014very"></a> K. Simonyan and A. Zisserman. <a href="https://doi.org/10.48550/arXiv.1409.1556" target="_blank" rel="noreferrer"><em>Very Deep Convolutional Networks for Large-Scale Image Recognition</em></a> (Apr 2015), <a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="noreferrer">arXiv:1409.1556</a>.</p></li><li><p><a id="trockman2022patches"></a> A. Trockman and J. Z. Kolter. <a href="https://doi.org/10.48550/arXiv.2201.09792" target="_blank" rel="noreferrer"><em>Patches Are All You Need?</em></a> (Jan 2022), <a href="https://arxiv.org/abs/2201.09792" target="_blank" rel="noreferrer">arXiv:2201.09792</a>.</p></li><li><p><a id="huang2017densely"></a> G. Huang, Z. Liu, L. Van Der Maaten and K. Q. Weinberger. <a href="https://doi.org/10.1109/cvpr.2017.243" target="_blank" rel="noreferrer"><em>Densely Connected Convolutional Networks</em></a>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> (IEEE, Honolulu, Jul 2017); pp. 4700–4708.</p></li><li><p><a id="szegedy2015going"></a> C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke and A. Rabinovich. <a href="https://doi.org/10.1109/cvpr.2015.7298594" target="_blank" rel="noreferrer"><em>Going Deeper with Convolutions</em></a>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (IEEE, Boston, MA, USA, Jun 2015); pp. 1–9.</p></li><li><p><a id="howard2017mobilenets"></a> A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto and H. Adam. <a href="https://doi.org/10.48550/arXiv.1704.04861" target="_blank" rel="noreferrer"><em>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</em></a> (Apr 2017), <a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noreferrer">arXiv:1704.04861</a>.</p></li><li><p><a id="sandler2018mobilenetv2"></a> M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L.-C. Chen. <a href="https://doi.org/10.1109/cvpr.2018.00474" target="_blank" rel="noreferrer"><em>MobileNetV2: Inverted Residuals and Linear Bottlenecks</em></a>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (IEEE, Salt Lake City, UT, Jun 2018); pp. 4510–4520.</p></li><li><p><a id="howard2019searching"></a> A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan, H. Adam and Q. Le. <a href="https://doi.org/10.1109/iccv.2019.00140" target="_blank" rel="noreferrer"><em>Searching for MobileNetV3</em></a>. In: <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</em> (IEEE, Seoul, Oct 2019); pp. 1314–1324.</p></li><li><p><a id="he2016deep"></a> K. He, X. Zhang, S. Ren and J. Sun. <a href="https://doi.org/10.1109/cvpr.2016.90" target="_blank" rel="noreferrer"><em>Deep Residual Learning for Image Recognition</em></a>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em> (IEEE, Las Vegas, Jun 2016); pp. 770–778.</p></li><li><p><a id="xie2017aggregated"></a> S. Xie, R. Girshick, P. Dollár, Z. Tu and K. He. <a href="https://doi.org/10.1109/cvpr.2017.634" target="_blank" rel="noreferrer"><em>Aggregated Residual Transformations for Deep Neural Networks</em></a>. In: <em>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</em> (IEEE, Honolulu, HI, Jul 2017); pp. 1492–1500.</p></li><li><p><a id="iandola2016squeezenetalexnetlevelaccuracy50x"></a> F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally and K. Keutzer. <a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="noreferrer"><em>SqueezeNet: AlexNet-level Accuracy with 50x Fewer Parameters and $&lt;$0.5MB Model Size</em></a> (<a href="https://doi.org/10.48550/arXiv.1602.07360" target="_blank" rel="noreferrer">Nov 2016</a>), <a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="noreferrer">arXiv:1602.07360 [cs.CV]</a>.</p></li><li><p><a id="zagoruyko2017wideresidualnetworks"></a> S. Zagoruyko and N. Komodakis. <a href="https://doi.org/10.48550/arXiv.1605.07146" target="_blank" rel="noreferrer"><em>Wide Residual Networks</em></a>, <a href="https://arxiv.org/abs/1605.07146" target="_blank" rel="noreferrer">arXiv:1605.07146 [cs.CV]</a>.</p></li></ol>',2)])])}const m=r(t,[["render",i]]);export{h as __pageData,m as default};
