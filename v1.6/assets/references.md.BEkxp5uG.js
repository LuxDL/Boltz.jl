import{_ as n,c as a,o as r,al as i}from"./chunks/framework.CHQizuvZ.js";const g=JSON.parse('{"title":"References","description":"","frontmatter":{},"headers":[],"relativePath":"references.md","filePath":"references.md","lastUpdated":null}'),o={name:"references.md"};function t(l,e,s,p,d,c){return r(),a("div",null,e[0]||(e[0]=[i('<h1 id="References" tabindex="-1">References <a class="header-anchor" href="#References" aria-label="Permalink to &quot;References {#References}&quot;">​</a></h1><ol><li><p><a id="greydanus2019hamiltonian"></a> S. Greydanus, M. Dzamba and J. Yosinski. <em>Hamiltonian neural networks</em>. Advances in neural information processing systems <strong>32</strong> (2019).</p></li><li><p><a id="luo2025transolver++"></a> H. Luo, H. Wu, H. Zhou, L. Xing, Y. Di, J. Wang and M. Long. <em>Transolver++: An Accurate Neural Solver for PDEs on Million-Scale Geometries</em>, arXiv preprint arXiv:2502.02414 (2025).</p></li><li><p><a id="wu2024transolver"></a> H. Wu, H. Luo, H. Wang, J. Wang and M. Long. <em>Transolver: A fast transformer solver for pdes on general geometries</em>, arXiv preprint arXiv:2402.02366 (2024).</p></li><li><p><a id="gaby2022lyapunovnetdeepneuralnetwork"></a> N. Gaby, F. Zhang and X. Ye. <a href="https://arxiv.org/abs/2109.13359" target="_blank" rel="noreferrer"><em>Lyapunov-Net: A Deep Neural Network Architecture for Lyapunov Function Approximation</em></a> (2022), <a href="https://arxiv.org/abs/2109.13359" target="_blank" rel="noreferrer">arXiv:2109.13359 [cs.LG]</a>.</p></li><li><p><a id="dosovitskiy2020image"></a> A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly and others. <em>An image is worth 16x16 words: Transformers for image recognition at scale</em>, arXiv preprint arXiv:2010.11929 (2020).</p></li><li><p><a id="krizhevsky2012imagenet"></a> A. Krizhevsky, I. Sutskever and G. E. Hinton. <em>Imagenet classification with deep convolutional neural networks</em>. Advances in neural information processing systems <strong>25</strong> (2012).</p></li><li><p><a id="tan2019efficientnet"></a> M. Tan and Q. Le. <em>Efficientnet: Rethinking model scaling for convolutional neural networks</em>. In: <em>International conference on machine learning</em> (PMLR, 2019); pp. 6105–6114.</p></li><li><p><a id="simonyan2014very"></a> K. Simonyan. <em>Very deep convolutional networks for large-scale image recognition</em>, arXiv preprint arXiv:1409.1556 (2014).</p></li><li><p><a id="trockman2022patches"></a> A. Trockman and J. Z. Kolter. <em>Patches are all you need?</em> arXiv preprint arXiv:2201.09792 (2022).</p></li><li><p><a id="huang2017densely"></a> G. Huang, Z. Liu, L. Van Der Maaten and K. Q. Weinberger. <em>Densely connected convolutional networks</em>. In: <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (2017); pp. 4700–4708.</p></li><li><p><a id="szegedy2015going"></a> C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke and A. Rabinovich. <em>Going deeper with convolutions</em>. In: <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (2015); pp. 1–9.</p></li><li><p><a id="howard2017mobilenets"></a> A. G. Howard. <em>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications</em>, arXiv preprint arXiv:1704.04861 (2017).</p></li><li><p><a id="sandler2018mobilenetv2"></a> M. Sandler, A. Howard, M. Zhu, A. Zhmoginov and L.-C. Chen. <em>Mobilenetv2: Inverted residuals and linear bottlenecks</em>. In: <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (2018); pp. 4510–4520.</p></li><li><p><a id="howard2019searching"></a> A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y. Zhu, R. Pang, V. Vasudevan and others. <em>Searching for mobilenetv3</em>. In: <em>Proceedings of the IEEE/CVF international conference on computer vision</em> (2019); pp. 1314–1324.</p></li><li><p><a id="he2016deep"></a> K. He, X. Zhang, S. Ren and J. Sun. <em>Deep residual learning for image recognition</em>. In: <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (2016); pp. 770–778.</p></li><li><p><a id="xie2017aggregated"></a> S. Xie, R. Girshick, P. Dollár, Z. Tu and K. He. <em>Aggregated residual transformations for deep neural networks</em>. In: <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (2017); pp. 1492–1500.</p></li><li><p><a id="iandola2016squeezenetalexnetlevelaccuracy50x"></a> F. N. Iandola, S. Han, M. W. Moskewicz, K. Ashraf, W. J. Dally and K. Keutzer. <a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="noreferrer"><em>SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</em></a> (2016), <a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="noreferrer">arXiv:1602.07360 [cs.CV]</a>.</p></li><li><p><a id="zagoruyko2017wideresidualnetworks"></a> S. Zagoruyko and N. Komodakis. <a href="https://arxiv.org/abs/1605.07146" target="_blank" rel="noreferrer"><em>Wide Residual Networks</em></a> (2017), <a href="https://arxiv.org/abs/1605.07146" target="_blank" rel="noreferrer">arXiv:1605.07146 [cs.CV]</a>.</p></li></ol>',2)]))}const u=n(o,[["render",t]]);export{g as __pageData,u as default};
