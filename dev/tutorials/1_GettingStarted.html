<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Getting Started | Boltz.jl Docs</title>
    <meta name="description" content="Documentation for Boltz.jl">
    <meta name="generator" content="VitePress v1.6.4">
    <link rel="preload stylesheet" href="/Boltz.jl/dev/assets/style.Ch2YsQZL.css" as="style">
    <link rel="preload stylesheet" href="/Boltz.jl/dev/vp-icons.css" as="style">
    
    <script type="module" src="/Boltz.jl/dev/assets/app.CprjO-Xr.js"></script>
    <link rel="preload" href="/Boltz.jl/dev/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/Boltz.jl/dev/assets/chunks/theme.CwTeq-Rp.js">
    <link rel="modulepreload" href="/Boltz.jl/dev/assets/chunks/framework.CZGTnI6-.js">
    <link rel="modulepreload" href="/Boltz.jl/dev/assets/tutorials_1_GettingStarted.md.BxY56UII.lean.js">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-Q8GYTEVTZ2"></script>
    <script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-Q8GYTEVTZ2");</script>
    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="icon" href="/favicon.ico">
    <link rel="manifest" href="/site.webmanifest">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-a9a9e638><!--[--><!--]--><!--[--><span tabindex="-1" data-v-492508fc></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-492508fc>Skip to content</a><!--]--><!----><header class="VPNav" data-v-a9a9e638 data-v-f1e365da><div class="VPNavBar" data-v-f1e365da data-v-822684d1><div class="wrapper" data-v-822684d1><div class="container" data-v-822684d1><div class="title" data-v-822684d1><div class="VPNavBarTitle has-sidebar" data-v-822684d1 data-v-0f4f798b><a class="title" href="/Boltz.jl/dev/" data-v-0f4f798b><!--[--><!--]--><!--[--><!--[--><!--[--><img class="VPImage dark logo" src="/Boltz.jl/dev/lux-logo-dark.svg" alt data-v-35a7d0b8><!--]--><!--[--><img class="VPImage light logo" src="/Boltz.jl/dev/lux-logo.svg" alt data-v-35a7d0b8><!--]--><!--]--><!--]--><span data-v-0f4f798b>Boltz.jl Docs</span><!--[--><!--]--></a></div></div><div class="content" data-v-822684d1><div class="content-body" data-v-822684d1><!--[--><!--]--><div class="VPNavBarSearch search" data-v-822684d1><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-822684d1 data-v-e6d46098><span id="main-nav-aria-label" class="visually-hidden" data-v-e6d46098> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/Boltz.jl/dev/index" tabindex="0" data-v-e6d46098 data-v-956ec74c><!--[--><span data-v-956ec74c>Boltz.jl</span><!--]--></a><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup active" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>Tutorials</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link active" href="/Boltz.jl/dev/tutorials/1_GettingStarted" data-v-acbfed09><!--[--><span data-v-acbfed09>Getting Started</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Boltz.jl/dev/tutorials/2_SymbolicOptimalControl" data-v-acbfed09><!--[--><span data-v-acbfed09>Symbolic Optimal Control</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="VPFlyout VPNavBarMenuGroup" data-v-e6d46098 data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><!----><span data-v-04f5c5e9>API Reference</span><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><div class="items" data-v-7dd3104a><!--[--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Boltz.jl/dev/api/basis" data-v-acbfed09><!--[--><span data-v-acbfed09>Basis Functions</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Boltz.jl/dev/api/layers" data-v-acbfed09><!--[--><span data-v-acbfed09>Layers API</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Boltz.jl/dev/api/vision" data-v-acbfed09><!--[--><span data-v-acbfed09>Vision Models</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Boltz.jl/dev/api/piml" data-v-acbfed09><!--[--><span data-v-acbfed09>Physics-Informed Models</span><!--]--></a></div><!--]--><!--[--><div class="VPMenuLink" data-v-7dd3104a data-v-acbfed09><a class="VPLink link" href="/Boltz.jl/dev/api/private" data-v-acbfed09><!--[--><span data-v-acbfed09>Private API</span><!--]--></a></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-822684d1 data-v-af096f4a><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-af096f4a data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-822684d1 data-v-164c457f data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://github.com/LuxDL/Boltz.jl" aria-label="github" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="https://twitter.com/avikpal1410" aria-label="twitter" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-twitter"></span></a><a class="VPSocialLink no-icon" href="https://julialang.org/slack/" aria-label="slack" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-slack"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-822684d1 data-v-925effce data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-04f5c5e9><span class="vpi-more-horizontal icon" data-v-04f5c5e9></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--[--><!----><div class="group" data-v-925effce><div class="item appearance" data-v-925effce><p class="label" data-v-925effce>Appearance</p><div class="appearance-action" data-v-925effce><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-925effce data-v-e40a8bb6 data-v-4a1c76db><span class="check" data-v-4a1c76db><span class="icon" data-v-4a1c76db><!--[--><span class="vpi-sun sun" data-v-e40a8bb6></span><span class="vpi-moon moon" data-v-e40a8bb6></span><!--]--></span></span></button></div></div></div><div class="group" data-v-925effce><div class="item social-links" data-v-925effce><div class="VPSocialLinks social-links-list" data-v-925effce data-v-ee7a9424><!--[--><a class="VPSocialLink no-icon" href="https://github.com/LuxDL/Boltz.jl" aria-label="github" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-github"></span></a><a class="VPSocialLink no-icon" href="https://twitter.com/avikpal1410" aria-label="twitter" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-twitter"></span></a><a class="VPSocialLink no-icon" href="https://julialang.org/slack/" aria-label="slack" target="_blank" rel="noopener" data-v-ee7a9424 data-v-d26d30cb><span class="vpi-social-slack"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--[--><!--[--><!--[--><a target="_blank" data-decoration="★" title="46 GitHub stars" href="https://github.com/LuxDL/Boltz.jl" data-v-33e7fa0e><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" width="20" height="20" fill="currentColor" style="vertical-align:middle;margin-right:0.25rem;margin-left:0.5rem;" data-v-33e7fa0e><path d="M12 .297C5.375.297 0 5.673 0 12.3c0 5.292 3.438 9.8 8.207 11.387.6.11.793-.26.793-.577 0-.285-.01-1.04-.015-2.04-3.338.727-4.042-1.61-4.042-1.61-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.807 1.305 3.493.997.107-.774.42-1.305.762-1.605-2.665-.3-5.467-1.333-5.467-5.931 0-1.31.47-2.382 1.236-3.222-.123-.303-.535-1.52.117-3.166 0 0 1.01-.323 3.31 1.23.96-.267 1.98-.4 3-.405 1.02.005 2.04.138 3 .405 2.3-1.553 3.31-1.23 3.31-1.23.653 1.646.24 2.863.117 3.166.765.84 1.236 1.912 1.236 3.222 0 4.61-2.807 5.625-5.477 5.921.43.372.823 1.102.823 2.222 0 1.606-.015 2.902-.015 3.293 0 .32.192.693.8.577C20.565 22.1 24 17.588 24 12.297 24 5.673 18.627.297 12 .297z" data-v-33e7fa0e></path></svg><span data-v-33e7fa0e>0.0k</span></a><a class="mobile" target="_blank" title="46 GitHub stars" href="https://github.com/LuxDL/Boltz.jl" data-v-33e7fa0e><svg xmlns="http://www.w3.org/2000/svg" width="21" height="21" viewBox="0 0 21 21" fill="none" data-v-33e7fa0e><path d="M19.625 5.60534C18.7083 4.03477 17.4649 2.79135 15.8945 1.87479C14.3238 0.958185 12.6091 0.5 10.7492 0.5C8.88947 0.5 7.17422 0.958325 5.60388 1.87479C4.0333 2.7913 2.78997 4.03477 1.87332 5.60534C0.956814 7.17587 0.498535 8.89089 0.498535 10.7504C0.498535 12.984 1.15021 14.9926 2.4539 16.7766C3.75744 18.5607 5.44142 19.7952 7.50571 20.4803C7.746 20.5249 7.92388 20.4936 8.03954 20.387C8.15524 20.2804 8.21302 20.1467 8.21302 19.9868C8.21302 19.9601 8.21073 19.7199 8.20629 19.266C8.20171 18.8122 8.19956 18.4162 8.19956 18.0783L7.89256 18.1315C7.69682 18.1673 7.44989 18.1825 7.15178 18.1782C6.8538 18.174 6.54446 18.1428 6.22419 18.0847C5.90377 18.0272 5.60575 17.8937 5.32988 17.6846C5.05416 17.4755 4.85842 17.2018 4.74272 16.8639L4.60925 16.5568C4.52029 16.3523 4.38023 16.1251 4.18888 15.8761C3.99754 15.6269 3.80405 15.458 3.60831 15.369L3.51486 15.3021C3.45259 15.2577 3.39481 15.204 3.34138 15.1418C3.28799 15.0796 3.24802 15.0173 3.22132 14.955C3.19458 14.8926 3.21674 14.8414 3.28804 14.8012C3.35933 14.761 3.48817 14.7416 3.67512 14.7416L3.94196 14.7814C4.11993 14.8171 4.34007 14.9236 4.60266 15.1017C4.86511 15.2796 5.08085 15.5109 5.24994 15.7956C5.4547 16.1605 5.7014 16.4385 5.99072 16.6299C6.27982 16.8212 6.5713 16.9167 6.86488 16.9167C7.15846 16.9167 7.41203 16.8945 7.62567 16.8502C7.83908 16.8057 8.0393 16.7388 8.22625 16.6499C8.30633 16.0535 8.52437 15.5953 8.88017 15.275C8.37304 15.2217 7.9171 15.1414 7.51212 15.0347C7.10736 14.9278 6.6891 14.7544 6.25761 14.5139C5.82589 14.2738 5.46774 13.9756 5.18309 13.6198C4.89839 13.2639 4.66474 12.7966 4.48247 12.2183C4.3001 11.6399 4.20889 10.9726 4.20889 10.2163C4.20889 9.13941 4.56044 8.22304 5.26341 7.46665C4.93411 6.65705 4.96519 5.74947 5.35676 4.744C5.61482 4.66382 5.9975 4.72399 6.50463 4.92412C7.01186 5.12434 7.38323 5.29587 7.61912 5.43808C7.85502 5.58024 8.04402 5.70071 8.18642 5.79842C9.01411 5.56715 9.86825 5.45149 10.7491 5.45149C11.6299 5.45149 12.4843 5.56715 13.312 5.79842L13.8192 5.47823C14.166 5.26459 14.5756 5.06881 15.0469 4.89083C15.5185 4.71295 15.8791 4.66396 16.1284 4.74414C16.5286 5.74966 16.5643 6.65719 16.2349 7.46679C16.9378 8.22318 17.2895 9.13978 17.2895 10.2164C17.2895 10.9727 17.198 11.6421 17.0159 12.225C16.8336 12.808 16.5979 13.2749 16.3088 13.6265C16.0194 13.9781 15.659 14.274 15.2275 14.5141C14.7959 14.7544 14.3775 14.9278 13.9728 15.0347C13.5678 15.1415 13.1119 15.2219 12.6047 15.2752C13.0673 15.6755 13.2986 16.3073 13.2986 17.1704V19.9864C13.2986 20.1464 13.3542 20.2799 13.4656 20.3867C13.5768 20.4932 13.7524 20.5246 13.9927 20.4799C16.0573 19.7949 17.7413 18.5603 19.0448 16.7762C20.3481 14.9922 21 12.9837 21 10.75C20.9996 8.89075 20.541 7.17587 19.625 5.60534Z" fill="currentColor" data-v-33e7fa0e></path></svg></a><!--]--><div class="VPFlyout VPNolebaseEnhancedReadabilitiesMenu VPNolebaseEnhancedReadabilitiesMenuFlyout" aria-label="Enhanced Readability" role="menuitem" data-v-04f5c5e9><button type="button" class="button" aria-haspopup="true" aria-expanded="false" data-v-04f5c5e9><span class="text" data-v-04f5c5e9><span class="i-icon-park-outline:book-open option-icon" data-v-04f5c5e9></span><!----><span class="vpi-chevron-down text-icon" data-v-04f5c5e9></span></span></button><div class="menu" data-v-04f5c5e9><div class="VPMenu" data-v-04f5c5e9 data-v-7dd3104a><!----><!--[--><!--]--></div></div></div><!--]--><!--]--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-822684d1 data-v-5dea55bf><span class="container" data-v-5dea55bf><span class="top" data-v-5dea55bf></span><span class="middle" data-v-5dea55bf></span><span class="bottom" data-v-5dea55bf></span></span></button></div></div></div></div><div class="divider" data-v-822684d1><div class="divider-line" data-v-822684d1></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-a9a9e638 data-v-070ab83d><div class="container" data-v-070ab83d><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-070ab83d><span class="vpi-align-left menu-icon" data-v-070ab83d></span><span class="menu-text" data-v-070ab83d>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-070ab83d data-v-168ddf5d><button data-v-168ddf5d>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-a9a9e638 data-v-18756405><div class="curtain" data-v-18756405></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-18756405><span class="visually-hidden" id="sidebar-aria-label" data-v-18756405> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0" data-v-9e426adc data-v-a4b0d9bf><!----><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Boltz.jl/dev/index" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Boltz.jl</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 collapsible has-active" data-v-9e426adc data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h2 class="text" data-v-a4b0d9bf>Tutorials</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Boltz.jl/dev/tutorials/1_GettingStarted" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Getting Started</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Boltz.jl/dev/tutorials/2_SymbolicOptimalControl" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Symbolic Optimal Control</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-9e426adc><section class="VPSidebarItem level-0 collapsible" data-v-9e426adc data-v-a4b0d9bf><div class="item" role="button" tabindex="0" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><h2 class="text" data-v-a4b0d9bf>API Reference</h2><div class="caret" role="button" aria-label="toggle section" tabindex="0" data-v-a4b0d9bf><span class="vpi-chevron-right caret-icon" data-v-a4b0d9bf></span></div></div><div class="items" data-v-a4b0d9bf><!--[--><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Boltz.jl/dev/api/basis" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Basis Functions</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Boltz.jl/dev/api/layers" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Layers API</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Boltz.jl/dev/api/vision" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Vision Models</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Boltz.jl/dev/api/piml" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Physics-Informed Models</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-a4b0d9bf data-v-a4b0d9bf><div class="item" data-v-a4b0d9bf><div class="indicator" data-v-a4b0d9bf></div><a class="VPLink link link" href="/Boltz.jl/dev/api/private" data-v-a4b0d9bf><!--[--><p class="text" data-v-a4b0d9bf>Private API</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-a9a9e638 data-v-91765379><div class="VPDoc has-sidebar has-aside" data-v-91765379 data-v-83890dd9><!--[--><!--]--><div class="container" data-v-83890dd9><div class="aside" data-v-83890dd9><div class="aside-curtain" data-v-83890dd9></div><div class="aside-container" data-v-83890dd9><div class="aside-content" data-v-83890dd9><div class="VPDocAside" data-v-83890dd9 data-v-6d7b3c46><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-6d7b3c46 data-v-b38bf2ff><div class="content" data-v-b38bf2ff><div class="outline-marker" data-v-b38bf2ff></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-b38bf2ff>On this page</div><ul class="VPDocOutlineItem root" data-v-b38bf2ff data-v-3f927ebe><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-6d7b3c46></div><!--[--><!--[--><!--[--><!--[--><!--[--><br><h2> Trusted by </h2><a class="enjoyer" href="https://sciml.ai/" target="_blank"><img width="32" height="32" src="https://avatars.githubusercontent.com/u/21238080?v=4"><span><p class="extra-info">Scientific Computing</p><p class="heading">SciML.ai</p><p class="extra-info">Machine Learning</p></span></a><!--]--><!--]--><!--]--><!--]--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-83890dd9><div class="content-container" data-v-83890dd9><!--[--><!--]--><main class="main" data-v-83890dd9><div style="position:relative;" class="vp-doc _Boltz_jl_dev_tutorials_1_GettingStarted" data-v-83890dd9><div><h1 id="Getting-Started" tabindex="-1">Getting Started <a class="header-anchor" href="#Getting-Started" aria-label="Permalink to &quot;Getting Started {#Getting-Started}&quot;">​</a></h1><div class="tip custom-block"><p class="custom-block-title">Prerequisites</p><p>Here we assume that you are familiar with <a href="https://lux.csail.mit.edu/stable/" target="_blank" rel="noreferrer"><code>Lux.jl</code></a>. If not please take a look at the <a href="https://lux.csail.mit.edu/stable/tutorials/" target="_blank" rel="noreferrer">Lux.jl tutoials</a>.</p></div><p><code>Boltz.jl</code> is just like <code>Lux.jl</code> but comes with more &quot;batteries included&quot;. Let&#39;s start by defining an MLP model.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Lux, Boltz, Random</span></span></code></pre></div><h2 id="Multi-Layer-Perceptron" tabindex="-1">Multi-Layer Perceptron <a class="header-anchor" href="#Multi-Layer-Perceptron" aria-label="Permalink to &quot;Multi-Layer Perceptron {#Multi-Layer-Perceptron}&quot;">​</a></h2><p>If we were to do this in <code>Lux.jl</code> we would write the following:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> Chain</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Dense</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">784</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">256</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, relu), </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Dense</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">256</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Chain(</span></span>
<span class="line"><span>    layer_1 = Dense(784 =&gt; 256, relu),            # 200_960 parameters</span></span>
<span class="line"><span>    layer_2 = Dense(256 =&gt; 10),                   # 2_570 parameters</span></span>
<span class="line"><span>)         # Total: 203_530 parameters,</span></span>
<span class="line"><span>          #        plus 0 states.</span></span></code></pre></div><p>But in <code>Boltz.jl</code> we can do this:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">model </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Layers</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">MLP</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">784</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, (</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">256</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">), relu)</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>MLP(</span></span>
<span class="line"><span>    chain = Chain(</span></span>
<span class="line"><span>        block1 = Dense(784 =&gt; 256, relu),         # 200_960 parameters</span></span>
<span class="line"><span>        block2 = Dense(256 =&gt; 10),                # 2_570 parameters</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>)         # Total: 203_530 parameters,</span></span>
<span class="line"><span>          #        plus 0 states.</span></span></code></pre></div><p>The <code>MLP</code> function is just a convenience wrapper around <code>Lux.Chain</code> that constructs a multi-layer perceptron with the given number of layers and activation function.</p><h2 id="How-about-VGG?" tabindex="-1">How about VGG? <a class="header-anchor" href="#How-about-VGG?" aria-label="Permalink to &quot;How about VGG? {#How-about-VGG?}&quot;">​</a></h2><p>Let&#39;s take a look at the <code>Vision</code> module. We can construct a VGG model with the following code:</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Vision</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">VGG</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">13</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>VGG(</span></span>
<span class="line"><span>    layer = Chain(</span></span>
<span class="line"><span>        feature_extractor = VGGFeatureExtractor(</span></span>
<span class="line"><span>            model = Chain(</span></span>
<span class="line"><span>                layer_1 = ConvNormActivation(</span></span>
<span class="line"><span>                    model = Chain(</span></span>
<span class="line"><span>                        block1 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 3 =&gt; 64, relu, pad=1),  # 1_792 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                        block2 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 64 =&gt; 64, relu, pad=1),  # 36_928 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_2 = MaxPool((2, 2)),</span></span>
<span class="line"><span>                layer_3 = ConvNormActivation(</span></span>
<span class="line"><span>                    model = Chain(</span></span>
<span class="line"><span>                        block1 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 64 =&gt; 128, relu, pad=1),  # 73_856 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                        block2 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 128 =&gt; 128, relu, pad=1),  # 147_584 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_4 = MaxPool((2, 2)),</span></span>
<span class="line"><span>                layer_5 = ConvNormActivation(</span></span>
<span class="line"><span>                    model = Chain(</span></span>
<span class="line"><span>                        block1 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 128 =&gt; 256, relu, pad=1),  # 295_168 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                        block2 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 256 =&gt; 256, relu, pad=1),  # 590_080 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_6 = MaxPool((2, 2)),</span></span>
<span class="line"><span>                layer_7 = ConvNormActivation(</span></span>
<span class="line"><span>                    model = Chain(</span></span>
<span class="line"><span>                        block1 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 256 =&gt; 512, relu, pad=1),  # 1_180_160 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                        block2 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 512 =&gt; 512, relu, pad=1),  # 2_359_808 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_8 = MaxPool((2, 2)),</span></span>
<span class="line"><span>                layer_9 = ConvNormActivation(</span></span>
<span class="line"><span>                    model = Chain(</span></span>
<span class="line"><span>                        block(1-2) = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 512 =&gt; 512, relu, pad=1),  # 4_719_616 (2_359_808 x 2) parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_10 = MaxPool((2, 2)),</span></span>
<span class="line"><span>            ),</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        classifier = VGGClassifier(</span></span>
<span class="line"><span>            model = Chain(</span></span>
<span class="line"><span>                layer_1 = Lux.FlattenLayer{Nothing}(nothing),</span></span>
<span class="line"><span>                layer_2 = Dense(25088 =&gt; 4096, relu),  # 102_764_544 parameters</span></span>
<span class="line"><span>                layer_3 = Dropout(0.5),</span></span>
<span class="line"><span>                layer_4 = Dense(4096 =&gt; 4096, relu),  # 16_781_312 parameters</span></span>
<span class="line"><span>                layer_5 = Dropout(0.5),</span></span>
<span class="line"><span>                layer_6 = Dense(4096 =&gt; 1000),    # 4_097_000 parameters</span></span>
<span class="line"><span>            ),</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>)         # Total: 133_047_848 parameters,</span></span>
<span class="line"><span>          #        plus 4 states.</span></span></code></pre></div><p>We can also load pretrained ImageNet weights using</p><div class="tip custom-block"><p class="custom-block-title">Load JLD2</p><p>You need to load <code>JLD2</code> before being able to load pretrained weights.</p></div><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> JLD2</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Vision</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">VGG</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">13</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">; pretrained</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">true</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>VGG(</span></span>
<span class="line"><span>    layer = Chain(</span></span>
<span class="line"><span>        feature_extractor = VGGFeatureExtractor(</span></span>
<span class="line"><span>            model = Chain(</span></span>
<span class="line"><span>                layer_1 = ConvNormActivation(</span></span>
<span class="line"><span>                    model = Chain(</span></span>
<span class="line"><span>                        block1 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 3 =&gt; 64, relu, pad=1),  # 1_792 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                        block2 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 64 =&gt; 64, relu, pad=1),  # 36_928 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_2 = MaxPool((2, 2)),</span></span>
<span class="line"><span>                layer_3 = ConvNormActivation(</span></span>
<span class="line"><span>                    model = Chain(</span></span>
<span class="line"><span>                        block1 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 64 =&gt; 128, relu, pad=1),  # 73_856 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                        block2 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 128 =&gt; 128, relu, pad=1),  # 147_584 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_4 = MaxPool((2, 2)),</span></span>
<span class="line"><span>                layer_5 = ConvNormActivation(</span></span>
<span class="line"><span>                    model = Chain(</span></span>
<span class="line"><span>                        block1 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 128 =&gt; 256, relu, pad=1),  # 295_168 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                        block2 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 256 =&gt; 256, relu, pad=1),  # 590_080 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_6 = MaxPool((2, 2)),</span></span>
<span class="line"><span>                layer_7 = ConvNormActivation(</span></span>
<span class="line"><span>                    model = Chain(</span></span>
<span class="line"><span>                        block1 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 256 =&gt; 512, relu, pad=1),  # 1_180_160 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                        block2 = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 512 =&gt; 512, relu, pad=1),  # 2_359_808 parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_8 = MaxPool((2, 2)),</span></span>
<span class="line"><span>                layer_9 = ConvNormActivation(</span></span>
<span class="line"><span>                    model = Chain(</span></span>
<span class="line"><span>                        block(1-2) = ConvNormActivationBlock(</span></span>
<span class="line"><span>                            block = Conv((3, 3), 512 =&gt; 512, relu, pad=1),  # 4_719_616 (2_359_808 x 2) parameters</span></span>
<span class="line"><span>                        ),</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_10 = MaxPool((2, 2)),</span></span>
<span class="line"><span>            ),</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        classifier = VGGClassifier(</span></span>
<span class="line"><span>            model = Chain(</span></span>
<span class="line"><span>                layer_1 = Lux.FlattenLayer{Nothing}(nothing),</span></span>
<span class="line"><span>                layer_2 = Dense(25088 =&gt; 4096, relu),  # 102_764_544 parameters</span></span>
<span class="line"><span>                layer_3 = Dropout(0.5),</span></span>
<span class="line"><span>                layer_4 = Dense(4096 =&gt; 4096, relu),  # 16_781_312 parameters</span></span>
<span class="line"><span>                layer_5 = Dropout(0.5),</span></span>
<span class="line"><span>                layer_6 = Dense(4096 =&gt; 1000),    # 4_097_000 parameters</span></span>
<span class="line"><span>            ),</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>)         # Total: 133_047_848 parameters,</span></span>
<span class="line"><span>          #        plus 4 states.</span></span></code></pre></div><h2 id="Loading-Models-from-Metalhead-Flux.jl" tabindex="-1">Loading Models from Metalhead (Flux.jl) <a class="header-anchor" href="#Loading-Models-from-Metalhead-Flux.jl" aria-label="Permalink to &quot;Loading Models from Metalhead (Flux.jl) {#Loading-Models-from-Metalhead-Flux.jl}&quot;">​</a></h2><p>We can load models from Metalhead (Flux.jl), just remember to load <code>Metalhead</code> before.</p><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> Metalhead</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Vision</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">ResNet</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">18</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>MetalheadWrapperLayer(</span></span>
<span class="line"><span>    layer = Chain(</span></span>
<span class="line"><span>        layer_1 = Chain(</span></span>
<span class="line"><span>            layer_1 = Chain(</span></span>
<span class="line"><span>                layer_1 = Conv((7, 7), 3 =&gt; 64, pad=3, stride=2, use_bias=false),  # 9_408 parameters</span></span>
<span class="line"><span>                layer_2 = BatchNorm(64, relu, affine=true, track_stats=true),  # 128 parameters, plus 129 non-trainable</span></span>
<span class="line"><span>                layer_3 = MaxPool((3, 3), pad=1, stride=2),</span></span>
<span class="line"><span>            ),</span></span>
<span class="line"><span>            layer_2 = Chain(</span></span>
<span class="line"><span>                layer_(1-2) = Parallel(</span></span>
<span class="line"><span>                    connection = addact(NNlib.relu, ...),</span></span>
<span class="line"><span>                    layer_1 = Lux.NoOpLayer(),</span></span>
<span class="line"><span>                    layer_2 = Chain(</span></span>
<span class="line"><span>                        layer_1 = Conv((3, 3), 64 =&gt; 64, pad=1, use_bias=false),  # 73_728 (36_864 x 2) parameters</span></span>
<span class="line"><span>                        layer_2 = BatchNorm(64, affine=true, track_stats=true),  # 256 (128 x 2) parameters, plus 258 (129 x 2) non-trainable</span></span>
<span class="line"><span>                        layer_3 = WrappedFunction(relu),</span></span>
<span class="line"><span>                        layer_4 = Conv((3, 3), 64 =&gt; 64, pad=1, use_bias=false),  # 73_728 (36_864 x 2) parameters</span></span>
<span class="line"><span>                        layer_5 = BatchNorm(64, affine=true, track_stats=true),  # 256 (128 x 2) parameters, plus 258 (129 x 2) non-trainable</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>            ),</span></span>
<span class="line"><span>            layer_3 = Chain(</span></span>
<span class="line"><span>                layer_1 = Parallel(</span></span>
<span class="line"><span>                    connection = addact(NNlib.relu, ...),</span></span>
<span class="line"><span>                    layer_1 = Chain(</span></span>
<span class="line"><span>                        layer_1 = Conv((1, 1), 64 =&gt; 128, stride=2, use_bias=false),  # 8_192 parameters</span></span>
<span class="line"><span>                        layer_2 = BatchNorm(128, affine=true, track_stats=true),  # 256 parameters, plus 257 non-trainable</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                    layer_2 = Chain(</span></span>
<span class="line"><span>                        layer_1 = Conv((3, 3), 64 =&gt; 128, pad=1, stride=2, use_bias=false),  # 73_728 parameters</span></span>
<span class="line"><span>                        layer_2 = BatchNorm(128, affine=true, track_stats=true),  # 256 parameters, plus 257 non-trainable</span></span>
<span class="line"><span>                        layer_3 = WrappedFunction(relu),</span></span>
<span class="line"><span>                        layer_4 = Conv((3, 3), 128 =&gt; 128, pad=1, use_bias=false),  # 147_456 parameters</span></span>
<span class="line"><span>                        layer_5 = BatchNorm(128, affine=true, track_stats=true),  # 256 parameters, plus 257 non-trainable</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_2 = Parallel(</span></span>
<span class="line"><span>                    connection = addact(NNlib.relu, ...),</span></span>
<span class="line"><span>                    layer_1 = Lux.NoOpLayer(),</span></span>
<span class="line"><span>                    layer_2 = Chain(</span></span>
<span class="line"><span>                        layer_1 = Conv((3, 3), 128 =&gt; 128, pad=1, use_bias=false),  # 147_456 parameters</span></span>
<span class="line"><span>                        layer_2 = BatchNorm(128, affine=true, track_stats=true),  # 256 parameters, plus 257 non-trainable</span></span>
<span class="line"><span>                        layer_3 = WrappedFunction(relu),</span></span>
<span class="line"><span>                        layer_4 = Conv((3, 3), 128 =&gt; 128, pad=1, use_bias=false),  # 147_456 parameters</span></span>
<span class="line"><span>                        layer_5 = BatchNorm(128, affine=true, track_stats=true),  # 256 parameters, plus 257 non-trainable</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>            ),</span></span>
<span class="line"><span>            layer_4 = Chain(</span></span>
<span class="line"><span>                layer_1 = Parallel(</span></span>
<span class="line"><span>                    connection = addact(NNlib.relu, ...),</span></span>
<span class="line"><span>                    layer_1 = Chain(</span></span>
<span class="line"><span>                        layer_1 = Conv((1, 1), 128 =&gt; 256, stride=2, use_bias=false),  # 32_768 parameters</span></span>
<span class="line"><span>                        layer_2 = BatchNorm(256, affine=true, track_stats=true),  # 512 parameters, plus 513 non-trainable</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                    layer_2 = Chain(</span></span>
<span class="line"><span>                        layer_1 = Conv((3, 3), 128 =&gt; 256, pad=1, stride=2, use_bias=false),  # 294_912 parameters</span></span>
<span class="line"><span>                        layer_2 = BatchNorm(256, affine=true, track_stats=true),  # 512 parameters, plus 513 non-trainable</span></span>
<span class="line"><span>                        layer_3 = WrappedFunction(relu),</span></span>
<span class="line"><span>                        layer_4 = Conv((3, 3), 256 =&gt; 256, pad=1, use_bias=false),  # 589_824 parameters</span></span>
<span class="line"><span>                        layer_5 = BatchNorm(256, affine=true, track_stats=true),  # 512 parameters, plus 513 non-trainable</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_2 = Parallel(</span></span>
<span class="line"><span>                    connection = addact(NNlib.relu, ...),</span></span>
<span class="line"><span>                    layer_1 = Lux.NoOpLayer(),</span></span>
<span class="line"><span>                    layer_2 = Chain(</span></span>
<span class="line"><span>                        layer_1 = Conv((3, 3), 256 =&gt; 256, pad=1, use_bias=false),  # 589_824 parameters</span></span>
<span class="line"><span>                        layer_2 = BatchNorm(256, affine=true, track_stats=true),  # 512 parameters, plus 513 non-trainable</span></span>
<span class="line"><span>                        layer_3 = WrappedFunction(relu),</span></span>
<span class="line"><span>                        layer_4 = Conv((3, 3), 256 =&gt; 256, pad=1, use_bias=false),  # 589_824 parameters</span></span>
<span class="line"><span>                        layer_5 = BatchNorm(256, affine=true, track_stats=true),  # 512 parameters, plus 513 non-trainable</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>            ),</span></span>
<span class="line"><span>            layer_5 = Chain(</span></span>
<span class="line"><span>                layer_1 = Parallel(</span></span>
<span class="line"><span>                    connection = addact(NNlib.relu, ...),</span></span>
<span class="line"><span>                    layer_1 = Chain(</span></span>
<span class="line"><span>                        layer_1 = Conv((1, 1), 256 =&gt; 512, stride=2, use_bias=false),  # 131_072 parameters</span></span>
<span class="line"><span>                        layer_2 = BatchNorm(512, affine=true, track_stats=true),  # 1_024 parameters, plus 1_025 non-trainable</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                    layer_2 = Chain(</span></span>
<span class="line"><span>                        layer_1 = Conv((3, 3), 256 =&gt; 512, pad=1, stride=2, use_bias=false),  # 1_179_648 parameters</span></span>
<span class="line"><span>                        layer_2 = BatchNorm(512, affine=true, track_stats=true),  # 1_024 parameters, plus 1_025 non-trainable</span></span>
<span class="line"><span>                        layer_3 = WrappedFunction(relu),</span></span>
<span class="line"><span>                        layer_4 = Conv((3, 3), 512 =&gt; 512, pad=1, use_bias=false),  # 2_359_296 parameters</span></span>
<span class="line"><span>                        layer_5 = BatchNorm(512, affine=true, track_stats=true),  # 1_024 parameters, plus 1_025 non-trainable</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>                layer_2 = Parallel(</span></span>
<span class="line"><span>                    connection = addact(NNlib.relu, ...),</span></span>
<span class="line"><span>                    layer_1 = Lux.NoOpLayer(),</span></span>
<span class="line"><span>                    layer_2 = Chain(</span></span>
<span class="line"><span>                        layer_1 = Conv((3, 3), 512 =&gt; 512, pad=1, use_bias=false),  # 2_359_296 parameters</span></span>
<span class="line"><span>                        layer_2 = BatchNorm(512, affine=true, track_stats=true),  # 1_024 parameters, plus 1_025 non-trainable</span></span>
<span class="line"><span>                        layer_3 = WrappedFunction(relu),</span></span>
<span class="line"><span>                        layer_4 = Conv((3, 3), 512 =&gt; 512, pad=1, use_bias=false),  # 2_359_296 parameters</span></span>
<span class="line"><span>                        layer_5 = BatchNorm(512, affine=true, track_stats=true),  # 1_024 parameters, plus 1_025 non-trainable</span></span>
<span class="line"><span>                    ),</span></span>
<span class="line"><span>                ),</span></span>
<span class="line"><span>            ),</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>        layer_2 = Chain(</span></span>
<span class="line"><span>            layer_1 = AdaptiveMeanPool((1, 1)),</span></span>
<span class="line"><span>            layer_2 = WrappedFunction(flatten),</span></span>
<span class="line"><span>            layer_3 = Dense(512 =&gt; 1000),         # 513_000 parameters</span></span>
<span class="line"><span>        ),</span></span>
<span class="line"><span>    ),</span></span>
<span class="line"><span>)         # Total: 11_689_512 parameters,</span></span>
<span class="line"><span>          #        plus 9_620 states.</span></span></code></pre></div><h2 id="Appendix" tabindex="-1">Appendix <a class="header-anchor" href="#Appendix" aria-label="Permalink to &quot;Appendix {#Appendix}&quot;">​</a></h2><div class="language-julia vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">julia</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">using</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> InteractiveUtils</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">InteractiveUtils</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">versioninfo</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> @isdefined</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(MLDataDevices)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> @isdefined</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(CUDA) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&amp;&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MLDataDevices</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">functional</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(CUDADevice)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        println</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        CUDA</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">versioninfo</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    end</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    if</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> @isdefined</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(AMDGPU) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&amp;&amp;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MLDataDevices</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">functional</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(AMDGPUDevice)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        println</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        AMDGPU</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">versioninfo</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    end</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">end</span></span></code></pre></div><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Julia Version 1.12.5</span></span>
<span class="line"><span>Commit 5fe89b8ddc1 (2026-02-09 16:05 UTC)</span></span>
<span class="line"><span>Build Info:</span></span>
<span class="line"><span>  Official https://julialang.org release</span></span>
<span class="line"><span>Platform Info:</span></span>
<span class="line"><span>  OS: Linux (x86_64-linux-gnu)</span></span>
<span class="line"><span>  CPU: 4 × AMD EPYC 7763 64-Core Processor</span></span>
<span class="line"><span>  WORD_SIZE: 64</span></span>
<span class="line"><span>  LLVM: libLLVM-18.1.7 (ORCJIT, znver3)</span></span>
<span class="line"><span>  GC: Built with stock GC</span></span>
<span class="line"><span>Threads: 1 default, 0 interactive, 1 GC (on 4 virtual cores)</span></span>
<span class="line"><span>Environment:</span></span>
<span class="line"><span>  JULIA_NUM_THREADS = 1</span></span>
<span class="line"><span>  JULIA_CUDA_HARD_MEMORY_LIMIT = 100%</span></span>
<span class="line"><span>  JULIA_PKG_PRECOMPILE_AUTO = 0</span></span>
<span class="line"><span>  JULIA_DEBUG = Literate</span></span></code></pre></div><hr><p><em>This page was generated using <a href="https://github.com/fredrikekre/Literate.jl" target="_blank" rel="noreferrer">Literate.jl</a>.</em></p></div></div></main><footer class="VPDocFooter" data-v-83890dd9 data-v-4f9813fa><!--[--><!--]--><div class="edit-info" data-v-4f9813fa><div class="edit-link" data-v-4f9813fa><a class="VPLink link vp-external-link-icon no-icon edit-link-button" href="https://github.com/LuxDL/Boltz.jl/edit/main/docs/src/tutorials/1_GettingStarted.md" target="_blank" rel="noreferrer" data-v-4f9813fa><!--[--><span class="vpi-square-pen edit-link-icon" data-v-4f9813fa></span> Edit this page on GitHub<!--]--></a></div><!----></div><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-4f9813fa><span class="visually-hidden" id="doc-footer-aria-label" data-v-4f9813fa>Pager</span><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link prev" href="/Boltz.jl/dev/index" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Previous page</span><span class="title" data-v-4f9813fa>Boltz.jl</span><!--]--></a></div><div class="pager" data-v-4f9813fa><a class="VPLink link pager-link next" href="/Boltz.jl/dev/tutorials/2_SymbolicOptimalControl" data-v-4f9813fa><!--[--><span class="desc" data-v-4f9813fa>Next page</span><span class="title" data-v-4f9813fa>Symbolic Optimal Control</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><footer class="VPFooter has-sidebar" data-v-a9a9e638 data-v-c970a860><div class="container" data-v-c970a860><p class="message" data-v-c970a860>Made with <a href="https://documenter.juliadocs.org/stable/" target="_blank"><strong>Documenter.jl</strong></a>, <a href="https://vitepress.dev" target="_blank"><strong>VitePress</strong></a> and <a href="https://luxdl.github.io/DocumenterVitepress.jl/stable" target="_blank"><strong>DocumenterVitepress.jl</strong></a><br>Released under the MIT License. Powered by the <a href="https://www.julialang.org">Julia Programming Language</a>.<br></p><p class="copyright" data-v-c970a860>© Copyright 2026 Avik Pal.</p></div></footer><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api_basis.md\":\"BFnxATPN\",\"api_layers.md\":\"B3srh2X0\",\"api_piml.md\":\"BjKYifiT\",\"api_private.md\":\"CAZOnCVa\",\"api_vision.md\":\"B0qBvym-\",\"index.md\":\"cqc8hqZn\",\"references.md\":\"CA2lO4OX\",\"tutorials_1_gettingstarted.md\":\"BxY56UII\",\"tutorials_2_symbolicoptimalcontrol.md\":\"Dpr2EjCM\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"Boltz.jl Docs\",\"description\":\"Documentation for Boltz.jl\",\"base\":\"/Boltz.jl/dev/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"outline\":\"deep\",\"logo\":{\"light\":\"/lux-logo.svg\",\"dark\":\"/lux-logo-dark.svg\"},\"search\":{\"provider\":\"local\",\"options\":{\"detailedView\":true}},\"nav\":[{\"text\":\"Boltz.jl\",\"link\":\"/index\"},{\"text\":\"Tutorials\",\"collapsed\":false,\"items\":[{\"text\":\"Getting Started\",\"link\":\"/tutorials/1_GettingStarted\"},{\"text\":\"Symbolic Optimal Control\",\"link\":\"/tutorials/2_SymbolicOptimalControl\"}]},{\"text\":\"API Reference\",\"collapsed\":false,\"items\":[{\"text\":\"Basis Functions\",\"link\":\"/api/basis\"},{\"text\":\"Layers API\",\"link\":\"/api/layers\"},{\"text\":\"Vision Models\",\"link\":\"/api/vision\"},{\"text\":\"Physics-Informed Models\",\"link\":\"/api/piml\"},{\"text\":\"Private API\",\"link\":\"/api/private\"}]}],\"sidebar\":[{\"text\":\"Boltz.jl\",\"link\":\"/index\"},{\"text\":\"Tutorials\",\"collapsed\":false,\"items\":[{\"text\":\"Getting Started\",\"link\":\"/tutorials/1_GettingStarted\"},{\"text\":\"Symbolic Optimal Control\",\"link\":\"/tutorials/2_SymbolicOptimalControl\"}]},{\"text\":\"API Reference\",\"collapsed\":false,\"items\":[{\"text\":\"Basis Functions\",\"link\":\"/api/basis\"},{\"text\":\"Layers API\",\"link\":\"/api/layers\"},{\"text\":\"Vision Models\",\"link\":\"/api/vision\"},{\"text\":\"Physics-Informed Models\",\"link\":\"/api/piml\"},{\"text\":\"Private API\",\"link\":\"/api/private\"}]}],\"editLink\":{\"pattern\":\"https://github.com/LuxDL/Boltz.jl/edit/main/docs/src/:path\",\"text\":\"Edit this page on GitHub\"},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/LuxDL/Boltz.jl\"},{\"icon\":\"twitter\",\"link\":\"https://twitter.com/avikpal1410\"},{\"icon\":\"slack\",\"link\":\"https://julialang.org/slack/\"}],\"footer\":{\"message\":\"Made with <a href=\\\"https://documenter.juliadocs.org/stable/\\\" target=\\\"_blank\\\"><strong>Documenter.jl</strong></a>, <a href=\\\"https://vitepress.dev\\\" target=\\\"_blank\\\"><strong>VitePress</strong></a> and <a href=\\\"https://luxdl.github.io/DocumenterVitepress.jl/stable\\\" target=\\\"_blank\\\"><strong>DocumenterVitepress.jl</strong></a><br>Released under the MIT License. Powered by the <a href=\\\"https://www.julialang.org\\\">Julia Programming Language</a>.<br>\",\"copyright\":\"© Copyright 2026 Avik Pal.\"},\"lastUpdated\":{\"text\":\"Updated at\",\"formatOptions\":{\"dateStyle\":\"full\",\"timeStyle\":\"medium\"}}},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":true}");</script>
    
  </body>
</html>